---
title: "TP3: Unsupervised Learning"
---
## Fecha de entrega: lunes 20/05/2024

## Resumen

El TP3 apunta a familiarizarse con algoritmos de aprendizaje no supervisado, tales como clustering, descomposición en componentes principales y algoritmos de reducción de la dimensionalidad. Esta guía tiene tres ejercicios solamente, de los cuales los primeros dos son entregables.

Encontrará los ejercicios con diferentes marcas:

-   ★: Ejercicio Obligatorio - no tenés opción.
-   ★★: Ejercicio recomendado - hacelo, que no te gane la timidez.
-   ★★★: Ejercicio avanzado - preguntate dos veces si querés entrar en el rabbit hole.
-   ★★★★: Ejercicio de integración - ***if you gaze long into an abyss, the abyss also gazes into you.***

### [Link al repositorio](https://classroom.github.com/a/FOcXgubK)

### Importante: 
Es probable que no funcionen los tests automáticos del autograding en github, estamos trabajando en solucionarlo.
Una vez que lo hayamos solucionado les avisaremos para que hagan un `pull` a su repositorio local con los cambios que hayamos efectuado; mientras tanto les recomendamos correr los tests de manera local con `pytest`, para obtener feedback automático. Mil disculpas, gracias por tanto.

## (★) 1. Clustering K-means

1. Implementa el algoritmo KMeans en el script `ej1/k_means.py`
2. Emplea el notebook `ej1/explore_train.ipynb` para explorar el archivo `ej1/data/synthetic_dataset_1.csv`. Posteriormente "entrena/fitea" el algoritmo KMeans que implementaste anteriormente para etiquetar los puntos según los K clusters.
3. Varía el número K para armar "the elbow curve" y elige el K óptimo (agrega tu respuesta en una celda).
4. Explora el archivo `data/synthetic_dataset_2.csv` y agrupa en clusters usando KMeans.
5. Utiliza el algoritmo DBSCAN (Density-Based Spatial Clustering of Applications with Noise) disponible en la librería sklearn para agrupar en clusters los datos (en el notebook `ej1/ej1_explore_train.ipynb`).

## (★) 2. PCA

0. Haz un breve EDA del dataset "Wine" de `sklearn` en el notebook `ej2/ej2_explore_train.ipynb`. Grafica la variable `target`para distintas combinaciones de 2 features (una en el eje horizontal y otra en el vertical). A ojímetro: ¿Puedes agrupar los tipos de vino fácilmente en algún subespacio de features? 
1. Tu tarea es completar la implementación de la clase PCA en el archivo `pca.py`.
2. Una vez que hayas completado la clase PCA, úsala para determinar los componentes principales del conjunto de datos "Wine" de `sklearn` en el notebook `ej2/ej2_explore_train.ipynb`.
3. Muestra e interpreta tus resultados. 
4. Determina de vuelta las componentes principales, pero esta vez usando la librería sklearn.
5. Obtener los loadings de las variables en los componentes principales. Hint:use `pca.components_`, Nos interesa saber los factores más importantes a la hora de elegir un vino!.
6. (Opcional) ¿Cómo podría agregarle esta funcionalidad a su implementación?

## (★★) 3. Embeddings y Reducción de la dimensionalidad 

### Recursos disponibles:

- **Modelo de Vocabulario**: Se proporciona un archivo de vectores FastText pre-entrenado en formato `.vec`. Puedes descargar el archivo de vectores desde el siguiente enlace: 

[Descargar vectores FastText en español](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz).

Una vez que tengas descargado el modelo de vectores, guardalo en la carpeta 'ej3/vec_model'.

- **Función `load_vectors` en el notebook `ej3/ej3.ipynb`**: Utiliza esta función proporcionada para cargar los embeddings de palabras, y continúa la implementación de las tareas asignadas en ese notebook.

### Tareas:

1. **Reducción de Dimensionalidad**:
   - Aplica un algoritmo de reducción de dimensionalidad para transformar los embeddings de palabras desde su espacio original a un espacio de 3 componentes. Puedes elegir entre t-SNE, o UMAP para esta tarea.
   - Guarda los resultados de la transformación para la visualización posterior.

2. **Visualización en 3D**:
   - Utiliza una herramienta de visualización dinámica como Plotly para crear un gráfico tridimensional interactivo de las palabras transformadas.
   - Cada punto en el gráfico debe estar etiquetado con la palabra correspondiente para facilitar la identificación.
   - Incluye ejes claramente marcados y proporciona una breve descripción de lo que representa cada eje.

3. **Interpretación**:
   - Analiza el gráfico y discute cómo las palabras con significados similares están agrupadas.
   - Reflexiona sobre si la reducción de dimensionalidad ha logrado capturar y visualizar relaciones semánticas entre palabras.
   - Identifica cualquier agrupación interesante o patrones inesperados y ofrece posibles explicaciones para estos hallazgos.